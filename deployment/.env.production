# Obstruction Server - Production Environment Configuration
# Copy this file to /opt/obstruction-server/.env and configure for your environment

# ============================================
# Server Configuration
# ============================================
# Port the application will listen on (should match nginx proxy_pass)
PORT=8081

# ============================================
# Flask Configuration
# ============================================
# Set to 'production' for production deployments, 'development' for debugging
FLASK_ENV=production

# Disable debug mode in production for security
# Set to True for detailed error messages and debug output
FLASK_DEBUG=False

# ============================================
# Logging Configuration
# ============================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Use DEBUG for detailed logs, INFO for production
LOG_LEVEL=INFO

# Enable detailed request/response logging (useful for debugging)
DEBUG_REQUESTS=False

# Log file path (leave empty to log only to console)
LOG_FILE=/var/log/obstruction-server/app.log

# Enable verbose gunicorn logging
GUNICORN_LOG_LEVEL=info

# ============================================
# Gunicorn Worker Configuration
# ============================================
# OPTIMIZED for CPU-bound obstruction calculations with 32 cores:
# - 32 workers = 1 worker per CPU core (best for CPU-intensive NumPy operations)
# - 2 threads = minimal threading (Python GIL limits parallelism for CPU work)
# - Total concurrent requests: 64 (32 workers Ã— 2 threads)
# - Each worker gets a full CPU core for numpy/scipy calculations
WORKERS=32

# Number of threads per worker
THREADS=2

# Request timeout in seconds (for long-running calculations)
TIMEOUT=900

# ============================================
# System Environment Variables
# ============================================
# Disable CUDA/GPU usage (set to -1 for CPU-only)
# If you have a GPU and want to use it, remove this or set to specific GPU ID
CUDA_VISIBLE_DEVICES=-1

# TensorFlow logging level (3 = ERROR only, reduces log noise)
TF_CPP_MIN_LOG_LEVEL=3

# Disable OpenEXR support in OpenCV (if not needed)
OPENCV_IO_ENABLE_OPENEXR=0

# Number of OpenMP threads (1 for single-threaded, or set to number of cores)
OMP_NUM_THREADS=1

# ============================================
# Application-Specific Configuration
# ============================================
# Add any custom application settings below

# Example: Maximum mesh size limit
# MAX_MESH_VERTICES=100000

# Example: Enable request logging
# ENABLE_REQUEST_LOGGING=True

# Example: API rate limiting
# RATE_LIMIT_ENABLED=False
# RATE_LIMIT_PER_MINUTE=60
